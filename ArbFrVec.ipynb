{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbc25d2",
   "metadata": {},
   "source": [
    "<h1 style= color:red;><b>Data Set</b> </h1>\n",
    "<p>Corpus: MultiUN</p>\n",
    "<p>Content: The MultiUN parallel corpus is extracted from the United Nations Website</p>\n",
    "<p>Sentences: 20.3M</p>\n",
    "<p>Link hugging face: <a href=\"https://huggingface.co/datasets/Helsinki-NLP/un_pc/viewer/ar-fr\">Link to data set</a> </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58689c40",
   "metadata": {},
   "source": [
    "<h1 style = color:red => <b>Imports</b><h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbc543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abdulazizalmakhdhoub/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# NLTK is a leading platform for building Python programs to work with human language data. \n",
    "import nltk\n",
    "\n",
    "\n",
    "#Library detecting the language used\n",
    "import langid\n",
    "\n",
    "# For calculating the duration of training\n",
    "import time\n",
    "\n",
    "# For stop word removal\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Convert a document into a list of tokens.\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# To shuffle the list of words randomly\n",
    "from random import shuffle\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6b25f",
   "metadata": {},
   "source": [
    "<h1 style= color:red> <b>Data set preperations</b><h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b1e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set_preperations():\n",
    "    # Use this dataset\n",
    "    ds = load_dataset(\"Helsinki-NLP/un_pc\",split='train', data_dir =\"ar-fr\",streaming = True)\n",
    "    return ds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c786d9c",
   "metadata": {},
   "source": [
    "<h1 style= color:red> <b>identifying ideal sentence  to keep the model as clean as possible</b><h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259de5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_ideal_sentence(ar,fr):\n",
    "\n",
    "    return langid.classify(ar)[0] == 'ar' and langid.classify(fr)[0] =='fr'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166fe55",
   "metadata": {},
   "source": [
    "<h1 style= color:red> <b>Resume logic </b><h1>\n",
    "<p> when colab runtime unexpectedly disconnects...the last index can be retrieved from output \n",
    "and affected to stopped_count argument so the training can restart where it left off (simply by passing already trained pairs) \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571304d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resume_logic(stopped_count,dataset):\n",
    "    # generator version of dataset\n",
    "    g = (iter(dataset))\n",
    "    for i in range(0,stopped_count):\n",
    "        next(g)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119e972",
   "metadata": {},
   "source": [
    "<h1 style= color:red> <b>Stop word removal </b><h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f99d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordsRemover(ar_list,fr_list):\n",
    "  ar_nS = []\n",
    "  fr_nS = []\n",
    "\n",
    "  ar_stopwords_list = stopwords.words('arabic')\n",
    "  fr_stopwords_list = stopwords.words('french')\n",
    " \n",
    "  for word in ar_list:\n",
    "    if word not in ar_stopwords_list:\n",
    "      ar_nS.append(word)\n",
    "\n",
    "  \n",
    "  for word in fr_list:\n",
    "    if word not in fr_stopwords_list:\n",
    "      fr_nS.append(word)\n",
    "  \n",
    "  return {\"ar\":ar_nS,\"fr\":fr_nS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b821575",
   "metadata": {},
   "source": [
    "<h1 style= color:red> <b>Arabic preprocessing</b><h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71042cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_preprocesser(line):\n",
    "  # remove commas and points\n",
    "  nLine = \"\"\n",
    "  for char in line:\n",
    "    if char not in [u'.', u'،']:\n",
    "      nLine += char\n",
    "  line = nLine\n",
    "  # remove_diacritics \n",
    "  regex = re.compile(r'[\\u064B\\u064C\\u064D\\u064E\\u064F\\u0650\\u0651\\u0652]')\n",
    "  line = re.sub(regex, '', line)\n",
    "\n",
    "  # remove_urls \n",
    "  regex = re.compile(r\"(http|https|ftp)://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "  line = re.sub(regex, ' ', line)\n",
    "  # remove elongation\n",
    "  regex = re.compile(r'\\u0640')\n",
    "  line = regex.sub('', line)\n",
    "  # remove_numbers \n",
    "  regex = re.compile(r\"(\\d|[\\u0660\\u0661\\u0662\\u0663\\u0664\\u0665\\u0666\\u0667\\u0668\\u0669])+\")\n",
    "  line = re.sub(regex, ' ', line)\n",
    "\n",
    "  # noramlize \n",
    "  regex = re.compile(r'[إأٱآا]')\n",
    "  line = re.sub(regex, 'ا', line)\n",
    "  regex = re.compile(r'ا+')\n",
    "  line = re.sub(regex, 'ا', line)\n",
    "  regex = re.compile(r'[ي]')\n",
    "  line = re.sub(regex, 'ى', line)\n",
    "  regex = re.compile(r'[ئ]')\n",
    "  line = re.sub(regex, 'ء', line)\n",
    "  regex = re.compile(r'[ؤ]')\n",
    "  line = re.sub(regex, 'و', line)\n",
    "  regex = re.compile(r'[ة]')\n",
    "  line = re.sub(regex, 'ه', line)\n",
    "  # remove one_character words\n",
    "  regex = re.compile(r'\\s.\\s')\n",
    "  line = re.sub(regex, ' ', line)\n",
    "  line = ' '.join([word for word in line.split() if not re.findall(r'[^\\s\\u0621\\u0622\\u0623\\u0624\\u0625\\u0626\\u0627\\u0628\\u0629\\u062A\\u062B\\u062C\\u062D\\u062E\\u062F\\u0630\\u0631\\u0632\\u0633\\u0634\\u0635\\u0636\\u0637\\u0638\\u0639\\u063A\\u0640\\u0641\\u0642\\u0643\\u0644\\u0645\\u0646\\u0647\\u0648\\u0649\\u064A]', word)])    \n",
    "  \n",
    "  return line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190a3ce",
   "metadata": {},
   "source": [
    "<h1 style= color:red> <b>random shuffle </b><h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b841161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffle(ar, fr):\n",
    "    # clean Arabic first\n",
    "    ar_clean = arabic_preprocesser(ar)\n",
    "\n",
    "    # Arabic list of words\n",
    "    ar_w_list = simple_preprocess(ar_clean)\n",
    "\n",
    "    # French list of words\n",
    "    fr_w_list = simple_preprocess(fr)\n",
    "\n",
    "    dic_ar_fr = stopWordsRemover(ar_w_list, fr_w_list)\n",
    "    temp = dic_ar_fr['ar'] + dic_ar_fr['fr']\n",
    "    shuffle(temp)\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd888a3",
   "metadata": {},
   "source": [
    "<h1 style= color:red> <b>Training</b><h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aafbe89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(modelLocation,re_train,stopped_count =0):\n",
    "    ds = data_set_preperations()\n",
    "    #number of rows\n",
    "    #pairsNumber = 20281645 in the original dataset\n",
    "    pairsNumber = 200000\n",
    "    g = Resume_logic(stopped_count,ds)\n",
    "    \n",
    "    documents = []\n",
    "    start = time.time()\n",
    "    for i in range(0, 33000):\n",
    "        row =next(g)['translation']\n",
    "        ar = row['ar']\n",
    "        fr = row['fr']\n",
    "        if(not identify_ideal_sentence(ar,fr)):\n",
    "            pass\n",
    "        else: \n",
    "            documents.append((random_shuffle(ar,fr)))\n",
    "    if (re_train == 0):\n",
    "        print(\"creating model\")\n",
    "        model = Word2Vec(documents, vector_size = 300, window = 5, min_count = 10, workers = 8, sg = 1)\n",
    "        model.save(modelLocation)\n",
    "        print(\"sentence {}: model initialized and trained on the suitable part of first 33000 sentence pairs, vocab now holds {} words\".format(i + 1 + stopped_count, len(model.wv)))\n",
    "    elif (re_train ==1):\n",
    "        print(\"loading model\")\n",
    "        model = Word2Vec.load(modelLocation)\n",
    "        model.build_vocab(corpus_iterable = documents, update = True)\n",
    "        model.train(documents,total_examples=len(documents),epochs=10)\n",
    "        model.save(modelLocation)\n",
    "        print(\"sentence {}: model loaded and trained on the suitable part of the other 33000 sentence pairs, vocab now holds {} words\".format(i + 1 + stopped_count,len(model.wv)))\n",
    "\n",
    "    documents = []\n",
    "    for i in range(0, pairsNumber - 33000 - stopped_count):\n",
    "        row =next(g)['translation']\n",
    "        ar = row['ar']\n",
    "        fr = row['fr']\n",
    "        if(not identify_ideal_sentence(ar,fr)):\n",
    "            pass\n",
    "        else: \n",
    "            documents.append((random_shuffle(ar,fr)))\n",
    "            if(len(documents)==33000):\n",
    "                model.build_vocab(corpus_iterable  = documents, update = True)\n",
    "                model.train(documents,total_examples=len(documents),epochs=10)\n",
    "                model.save(modelLocation)\n",
    "                print(\"sentence {}: model loaded and trained on the suitable part of the other 33000 sentence pairs, vocab now holds {} words\".format(i + 1 + 33000 + stopped_count,len(model.wv)))\n",
    "                documents = []\n",
    "    model.build_vocab(corpus_iterable = documents,update=True)\n",
    "    model.train(documents,total_examples=len(documents),epochs=10)\n",
    "    model.save(modelLocation)\n",
    "    print(\"sentence {}: model trained on the remaining suitable sentence pairs, vocab now holds {} words\".format(i + 1 + 33000 + stopped_count, len(model.wv)))\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"DONE :)\")\n",
    "    print(\"time spent in traning (in seconds): {}\".format(end-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7541d",
   "metadata": {},
   "source": [
    "<h1 style= color:red> <b>Test</b><h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18497cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebea7c7daf4c46a4b94a99170933f453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "sentence 33000: model initialized and trained on the suitable part of first 33000 sentence pairs, vocab now holds 9779 words\n",
      "sentence 71544: model loaded and trained on the suitable part of the other 33000 sentence pairs, vocab now holds 14417 words\n",
      "sentence 111404: model loaded and trained on the suitable part of the other 33000 sentence pairs, vocab now holds 16253 words\n",
      "sentence 147925: model loaded and trained on the suitable part of the other 33000 sentence pairs, vocab now holds 18464 words\n",
      "sentence 183395: model loaded and trained on the suitable part of the other 33000 sentence pairs, vocab now holds 19720 words\n",
      "sentence 200000: model trained on the remaining suitable sentence pairs, vocab now holds 19948 words\n",
      "DONE :)\n",
      "time spent in traning (in seconds): 902.062992811203\n"
     ]
    }
   ],
   "source": [
    "trainer(\"randomshuffle_5window_skipgram_300size.model\",re_train = 0,stopped_count =0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Arb_Fr_Vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
